{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Visualization With Scattertext\n",
    "## Jason S. Kessler @jasonkessler\n",
    "### Global AI Conference 2018, Seattle, WA. April 27, 2018.\n",
    "\n",
    "The Github repository for talk is at [https://github.com/JasonKessler/GlobalAI2018](https://github.com/JasonKessler/GlobalAI2018). \n",
    "\n",
    "Visualizations were made using [Scattertext](https://github.com/JasonKessler/scattertext).\n",
    "\n",
    "Please cite as: \n",
    "Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scattertext as st\n",
    "import spacy\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert st.__version__ >= '0.0.2.25'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    " \n",
    "Dataset consists of reviews of movies and plot descriptions.  Plot descriptions are guaranteed to be from a movie which was reviewed. \n",
    "\n",
    "Data set is from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "References:\n",
    "* Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan, Thumbs up? Sentiment Classification using Machine Learning Techniques, Proceedings of EMNLP 2002.\n",
    "\n",
    "* Bo Pang and Lillian Lee, A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, Proceedings of ACL 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive    2455\n",
      "Negative    2411\n",
      "Plot         156\n",
      "Name: category_name, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A senior at an elite college (Katie Holmes), a...</td>\n",
       "      <td>abandon</td>\n",
       "      <td>Plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Lightman is a hip Londoner who one day re...</td>\n",
       "      <td>about_a_boy</td>\n",
       "      <td>Plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Warren Schmidt (Nicholson) is forced to deal w...</td>\n",
       "      <td>about_schmidt</td>\n",
       "      <td>Plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An account of screenwriter Charlie Kaufman's (...</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>Plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ali G unwittingly becomes a pawn in the evil C...</td>\n",
       "      <td>ali_g_indahouse</td>\n",
       "      <td>Plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       movie_name  \\\n",
       "0  A senior at an elite college (Katie Holmes), a...          abandon   \n",
       "1  Will Lightman is a hip Londoner who one day re...      about_a_boy   \n",
       "2  Warren Schmidt (Nicholson) is forced to deal w...    about_schmidt   \n",
       "3  An account of screenwriter Charlie Kaufman's (...       adaptation   \n",
       "4  Ali G unwittingly becomes a pawn in the evil C...  ali_g_indahouse   \n",
       "\n",
       "  category_name  \n",
       "0          Plot  \n",
       "1          Plot  \n",
       "2          Plot  \n",
       "3          Plot  \n",
       "4          Plot  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = st.SampleCorpora.RottenTomatoes.get_data()\n",
    "rdf['category_name'] = rdf['category'].apply(lambda x: {'plot': 'Plot', 'rotten': 'Negative', 'fresh': 'Positive'}[x])\n",
    "print(rdf.category_name.value_counts())\n",
    "rdf[['text', 'movie_name', 'category_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (st.CorpusFromPandas(rdf, \n",
    "                              category_col='category_name', \n",
    "                              text_col='text',\n",
    "                              nlp = st.whitespace_nlp_with_sentences)\n",
    "          .build())\n",
    "corpus.get_term_freq_df().to_csv('term_freqs.csv')\n",
    "unigram_corpus = corpus.get_unigram_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the corpus using Scattertext\n",
    "\n",
    "The x-axis indicates the rank of a word or bigram in the set of positive reviews, and the y-axis negative reviews.\n",
    "\n",
    "Ranks are determined using \"dense\" ranking, meaning the most frequent terms, regardless of ties, are given rank 1, the next most frequent terms, regardless of ties, are given rank 2, etc.\n",
    "\n",
    "It appears that terms more associated with a class are a further distance from the diagonal line between the lower-left and upper-right corners.  Terms are colored according to this distance.  We'll return to this in a bit.\n",
    "\n",
    "Scattertext selectively labels points in such a way as to prevent labels from overlapping other elements of the graph. Mouse-over points and term labels for a preview, and click for a key-word in context view.\n",
    "\n",
    "References:\n",
    "* Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"700\"\n",
       "            src=\"rotten_fresh_stdense.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11d6eecf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='Positive',\n",
    "    not_categories=['Negative'],\n",
    "    sort_by_dist=False,\n",
    "    metadata=rdf['movie_name'],\n",
    "    term_scorer=st.RankDifference(),\n",
    "    transform=st.Scalers.percentile_dense\n",
    ")\n",
    "file_name = 'rotten_fresh_stdense.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1300, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We view can see more terms through breaking ties in ranking alphabetically.\n",
    "Lower frequency terms are more prominent in this view, and more terms can be labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"700\"\n",
       "            src=\"rotten_fresh_st.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11eaa9978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='Positive',\n",
    "    not_categories=['Negative'],\n",
    "    sort_by_dist=False,\n",
    "    metadata=rdf['movie_name'],\n",
    "    term_scorer=st.RankDifference(),\n",
    ")\n",
    "file_name = 'rotten_fresh_st.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1300, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled F-Score\n",
    "### Associatied terms have a *relatively* high category-specific precision and category-specific term frequency (i.e., % of terms in category are term)\n",
    "### Take the harmonic mean of precision and frequency (both have to be high)\n",
    "### We will make two adjustments to this method in order to come up with the final formulation of Scaled F-Score\n",
    "\n",
    "Given a word $w_i \\in W$ and a category $c_j \\in C$, define the precision of the word $w_i$ wrt to a category as:\n",
    "$$ \\mbox{prec}(i,j) = \\frac{\\#(w_i, c_j)}{\\sum_{c \\in C} \\#(w_i, c)}. $$\n",
    "\n",
    "The function $\\#(w_i, c_j)$ represents either the number of times $w_i$ occurs in a document labeled with the category $c_j$ or the number of documents labeled $c_j$ which contain $w_i$.\n",
    "\n",
    "Similarly, define the frequency a word occurs in the category as:\n",
    "\n",
    "$$ \\mbox{freq}(i, j) = \\frac{\\#(w_i, c_j)}{\\sum_{w \\in W} \\#(w, c_j)}. $$\n",
    "\n",
    "The harmonic mean of these two values of these two values is defined as:\n",
    "\n",
    "$$ \\mathcal{H}_\\beta(i,j) = (1 + \\beta^2) \\frac{\\mbox{prec}(i,j) \\cdot \\mbox{freq}(i,j)}{\\beta^2 \\cdot \\mbox{prec}(i,j) + \\mbox{freq}(i,j)}. $$\n",
    "\n",
    "$\\beta \\in \\mathcal{R}^+$ is a scaling factor where frequency is favored if $\\beta < 1$, precision if $\\beta > 1$, and both are equally weighted if $\\beta = 1$. F-Score is equivalent to the harmonic mean where $\\beta = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive freq</th>\n",
       "      <th>Negative freq</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>pos_freq_pct</th>\n",
       "      <th>pos_hmean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2346</td>\n",
       "      <td>2288</td>\n",
       "      <td>0.506258</td>\n",
       "      <td>0.048037</td>\n",
       "      <td>0.087748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1775</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.523908</td>\n",
       "      <td>0.036345</td>\n",
       "      <td>0.067975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1637</td>\n",
       "      <td>1179</td>\n",
       "      <td>0.581321</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.063385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1480</td>\n",
       "      <td>1235</td>\n",
       "      <td>0.545120</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0.057418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>942</td>\n",
       "      <td>1010</td>\n",
       "      <td>0.482582</td>\n",
       "      <td>0.019289</td>\n",
       "      <td>0.037095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>826</td>\n",
       "      <td>801</td>\n",
       "      <td>0.507683</td>\n",
       "      <td>0.016913</td>\n",
       "      <td>0.032736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>818</td>\n",
       "      <td>726</td>\n",
       "      <td>0.529793</td>\n",
       "      <td>0.016750</td>\n",
       "      <td>0.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>808</td>\n",
       "      <td>749</td>\n",
       "      <td>0.518947</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.032067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>676</td>\n",
       "      <td>622</td>\n",
       "      <td>0.520801</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.026967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>617</td>\n",
       "      <td>602</td>\n",
       "      <td>0.506153</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.024652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Positive freq  Negative freq  pos_precision  pos_freq_pct  pos_hmean\n",
       "term                                                                      \n",
       "the            2346           2288       0.506258      0.048037   0.087748\n",
       "a              1775           1613       0.523908      0.036345   0.067975\n",
       "and            1637           1179       0.581321      0.033520   0.063385\n",
       "of             1480           1235       0.545120      0.030305   0.057418\n",
       "to              942           1010       0.482582      0.019289   0.037095\n",
       "it              826            801       0.507683      0.016913   0.032736\n",
       "is              818            726       0.529793      0.016750   0.032473\n",
       "s               808            749       0.518947      0.016545   0.032067\n",
       "in              676            622       0.520801      0.013842   0.026967\n",
       "that            617            602       0.506153      0.012634   0.024652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import hmean\n",
    "\n",
    "term_freq_df = corpus.get_unigram_corpus().get_term_freq_df()[['Positive freq', 'Negative freq']]\n",
    "term_freq_df = term_freq_df[term_freq_df.sum(axis=1) > 0]\n",
    "\n",
    "term_freq_df['pos_precision'] = (term_freq_df['Positive freq'] * 1./\n",
    "                                 (term_freq_df['Positive freq'] + term_freq_df['Negative freq']))\n",
    "\n",
    "term_freq_df['pos_freq_pct'] = (term_freq_df['Positive freq'] * 1.\n",
    "                                /term_freq_df['Positive freq'].sum())\n",
    "\n",
    "term_freq_df['pos_hmean'] = (term_freq_df\n",
    "                             .apply(lambda x: (hmean([x['pos_precision'], x['pos_freq_pct']])\n",
    "                                               if x['pos_precision'] > 0 and x['pos_freq_pct'] > 0 \n",
    "                                               else 0), axis=1))\n",
    "term_freq_df.sort_values(by='pos_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12032.000000\n",
       "mean         0.000083\n",
       "std          0.000826\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000020\n",
       "75%          0.000041\n",
       "max          0.048037\n",
       "Name: pos_freq_pct, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_df.pos_freq_pct.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12032.000000\n",
       "mean         0.506651\n",
       "std          0.418623\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.500000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: pos_precision, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_df.pos_precision.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027233450048119254"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "log_freqs = np.log(term_freq_df.pos_freq_pct[term_freq_df.pos_freq_pct > 0])\n",
    "\n",
    "sns.distplot(log_freqs[:1000], kde=False, rug=True, hist=False, rug_kws={\"color\": \"k\"})\n",
    "\n",
    "x = np.linspace(log_freqs.min(), \n",
    "                log_freqs.max(), \n",
    "                100)\n",
    "y = norm(log_freqs.mean(), log_freqs.std()).pdf(x)\n",
    "plt.plot(x, y ,color='k')\n",
    "word_freq = np.log(term_freq_df.pos_freq_pct[term_freq_df.pos_freq_pct > 0].loc['bad'])\n",
    "plt.axvline(x=word_freq, color='red', label='Log frequency of \"bad\"')\n",
    "plt.fill_between(x[x < word_freq], \n",
    "                 y[x < word_freq], y[x < word_freq] * 0, \n",
    "                 facecolor='blue', \n",
    "                 alpha=0.5,\n",
    "                 label='''Log-normal CDF of bad's frequency''')\n",
    "ax.set_xlabel('Log term frequency')\n",
    "ax.set_ylabel('Cumulative term probability')\n",
    "plt.legend()\n",
    "for item in ([ax.title,  ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels() ):\n",
    "    item.set_fontsize(12)\n",
    "plt.rc('legend', fontsize=15)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
